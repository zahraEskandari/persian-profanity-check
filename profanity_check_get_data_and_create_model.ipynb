{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages, load config file and do twitter authentication\n",
    "import configparser \n",
    "import tweepy\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import preprocessing as pp\n",
    "#from sklearn import cross_validation\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.externals import joblib\n",
    "import random\n",
    "\n",
    "# get config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"config.ini\")\n",
    "\n",
    "# read twitter's 4 oauth elements from config file , assuming you have them!\n",
    "consumer_key = config.get('twitter', 'consumer_key' )\n",
    "consumer_secret = config.get('twitter', 'consumer_secret' )\n",
    "access_key = config.get('twitter', 'access_key' )\n",
    "access_secret = config.get('twitter', 'access_secret' )\n",
    "\n",
    "#twitter_authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "\n",
    "#get stopwords path\n",
    "stopwords_path = config.get('preprocessing', 'stopwords_path' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get bad tweets\n",
    "\n",
    "\n",
    "#function to get tweet text from tweepy status\n",
    "def get_tweetText(status) :   \n",
    "    if 'extended_tweet' in status._json:\n",
    "      return status.extended_tweet['full_text']\n",
    "    else:\n",
    "      return status.text\n",
    "\n",
    "# get tweets\n",
    "api = tweepy.API(auth)\n",
    "fa_bad_words_file = config.get('data_gathering'  , 'fa_swear_words_file')\n",
    "fa_bad_words = pd.read_csv(fa_bad_words_file , header=None)\n",
    "fa_bad_words.columns = ['word']\n",
    "max_tweets =  1#0 \n",
    "all_bad_tweets = []\n",
    "for bad_word in fa_bad_words['word'] : \n",
    "    try:\n",
    "        print ('--------------- getting tweets containing \"'+ bad_word + '\"------------------')\n",
    "        searched_tweets = [status for status in tweepy.Cursor(api.search, q=bad_word, lang = 'fa').items(max_tweets)]\n",
    "        tweets = [[get_tweetText(tweet)] for tweet in searched_tweets]\n",
    "        all_bad_tweets.extend(tweets)\n",
    "        time.sleep(30) # Delay for 5 seconds to avoid 'Max retries exceeded' error!\n",
    "    except Exception as err:\n",
    "        print (err)        \n",
    "        time.sleep(60) # in case of 'Max retries exceeded' error Delay for 60 seconds, then continue\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "# clean tweets from # and user ids \n",
    "all_bad_tweets_clean= [tweet[0].replace(\"#\" , \"\").replace(\",\" , \" \").replace(\"\\n\" , '') for tweet in all_bad_tweets ]\n",
    "all_bad_tweets_clean= [re.sub('RT @[^\\s]+','',tweet) for tweet in all_bad_tweets_clean ]\n",
    "all_bad_tweets_clean= [re.sub('@[^\\s]+','',tweet) for tweet in all_bad_tweets_clean ]\n",
    "\n",
    "# create a datafrme\n",
    "bad_tweets_df = pd.DataFrame(all_bad_tweets_clean)\n",
    "bad_tweets_df['isOffensive'] = bad_tweets_df.apply(lambda x: 1, axis=1) \n",
    "bad_tweets_df.columns= ['text' , 'isOffensive']\n",
    "\n",
    "# save results\n",
    "# if file does not exist write header \n",
    "bad_tweets_filename = config.get('twitter', 'bad_tweets_file' )\n",
    "#if file doesn't exist create it\n",
    "if not os.path.isfile(bad_tweets_filename):\n",
    "   bad_tweets_df.to_csv(bad_tweets_filename, header=False)\n",
    "else: # else it exists so append to it\n",
    "   bad_tweets_df.to_csv(bad_tweets_filename, mode='a', header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get good tweets \n",
    "\n",
    "good_users_file = config.get('data_gathering'  , 'good_users_file')\n",
    "good_users = pd.read_csv(good_users_file , header=None)\n",
    "good_users.columns = ['userId']\n",
    "users =good_users['userId'] \n",
    "    \n",
    "api = tweepy.API(auth)\n",
    "all_good_tweets =[] \n",
    "num_user_tweets = 1#00 \n",
    "for usr in users:\n",
    "    try:\n",
    "        print('--------------- getting tweets from \"'+ usr + '\" ------------------')\n",
    "        user_tweets = api.user_timeline(screen_name = usr , count = num_user_tweets, include_rts = True)\n",
    "        for tweet in user_tweets:\n",
    "            all_good_tweets.extend([get_tweetText(tweet)])\n",
    "    except Exception as e:\n",
    "        print (str(e))\n",
    "        continue\n",
    "        \n",
    "# clean tweets from # and user ids \n",
    "all_good_tweets_clean= [tweet[0].replace(\"#\" , \"\").replace(\",\" , \" \").replace(\"\\n\" , '') for tweet in all_good_tweets ]\n",
    "all_good_tweets_clean= [re.sub('RT @[^\\s]+','',tweet) for tweet in all_good_tweets_clean ]\n",
    "all_good_tweets_clean= [re.sub('@[^\\s]+','',tweet) for tweet in all_good_tweets_clean ]\n",
    "\n",
    "# create a datafrme\n",
    "good_tweets_df = pd.DataFrame(all_bad_tweets_clean)\n",
    "good_tweets_df['isOffensive'] = bad_tweets_df.apply(lambda x: 0, axis=1) \n",
    "good_tweets_df.columns= ['text' , 'isOffensive']\n",
    "\n",
    "# save results\n",
    "# if file does not exist write header \n",
    "good_tweets_filename = config.get('twitter', 'good_tweets_file' )\n",
    "#if file doesn't exist create it\n",
    "if not os.path.isfile(good_tweets_filename):\n",
    "   good_tweets_df.to_csv(good_tweets_filename, header=False)\n",
    "else: # else it exists so append to it\n",
    "   good_tweets_df.to_csv(good_tweets_filename, mode='a', header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read bad and good tweets from file \n",
    "bad_tweets = pd.read_csv(config.get('data_gathering'  , 'bad_tweets_file'))\n",
    "good_tweets = pd.read_csv(config.get('data_gathering'  , 'good_tweets_file'))\n",
    "    \n",
    "bad_tweets.columns = [ 'ID' ,'text' , 'isOffensive' ]\n",
    "good_tweets.columns = [ 'ID' , 'text' , 'isOffensive' ]\n",
    "        \n",
    "indexNames = bad_tweets[ bad_tweets['text'].str.len() == 0  ].index \n",
    "# Delete these row indexes from dataFrame\n",
    "bad_tweets.drop(indexNames , inplace=True)    \n",
    "bad_tweets = bad_tweets[bad_tweets['text'].notnull()]\n",
    "    \n",
    "    \n",
    "indexNames = good_tweets[ good_tweets['text'].str.len() == 0  ].index \n",
    "# Delete these row indexes from dataFrame\n",
    "good_tweets.drop(indexNames , inplace=True)    \n",
    "good_tweets = good_tweets[bad_tweets['text'].notnull()]\n",
    "    \n",
    "    \n",
    "    \n",
    "tweets_df = pd.concat([bad_tweets, good_tweets])    \n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs = tweets_df[\"text\"].values\n",
    "tweets_df[\"preprocessed\"] = pp.cleanText(docs , stopwords_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into training and test sets \n",
    "\n",
    "def shuffle(df):\n",
    "    index = list(df.index)\n",
    "    random.shuffle(index)\n",
    "    df = df.ix[index]\n",
    "    df.reset_index()\n",
    "    return df\n",
    "\n",
    "#shuffle rows \n",
    "tweets_df = shuffle(tweets_df)\n",
    "\n",
    "#get test and train sets\n",
    "msk = np.random.rand(len(tweets_df)) < 0.8\n",
    "\n",
    "train_tweets_df = tweets_df[msk]\n",
    "test_tweets_df = tweets_df[~msk]\n",
    "\n",
    "print('number of train examples : ' +  str(len(train_tweets_df)))\n",
    "print('number of train examples : '  + str(len(test_tweets_df)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv(stopwords_path, delimiter=',')\n",
    "stop_words = df[\"stopwords\"].values\n",
    "\n",
    "texts = [ ' '.join(tweet_tokens)  for tweet_tokens in  train_tweets_df[\"preprocessed\"] ]\n",
    "y = train_tweets_df['isOffensive'].tolist()\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words.tolist(), min_df=0.0000001)\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Train the model\n",
    "model = LinearSVC(class_weight=\"balanced\", dual=False, tol=1e-2, max_iter=1e5)\n",
    "cclf = CalibratedClassifierCV(base_estimator=model)\n",
    "cclf.fit(X, y)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(vectorizer, 'vectorizer.joblib')\n",
    "joblib.dump(cclf, 'model.joblib') \n",
    "\n",
    "#vectorizer = joblib.load(pkg_resources.resource_filename('profanity_check', 'data/vectorizer.joblib'))\n",
    "#model = joblib.load(pkg_resources.resource_filename('profanity_check', 'data/model.joblib'))\n",
    "import numpy as np\n",
    "model = cclf\n",
    "def _get_profane_prob(prob):\n",
    "  return prob[1]\n",
    "\n",
    "def predict(texts):\n",
    "  return model.predict(vectorizer.transform(texts))\n",
    "\n",
    "def predict_prob(texts):\n",
    "  return np.apply_along_axis(_get_profane_prob, 1, model.predict_proba(vectorizer.transform(texts)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame(  [ test_tweets_df['isOffensive'].tolist()  \n",
    "                    , predict_prob( [ ' '.join(tweet_tokens)  for tweet_tokens in  test_tweets_df[\"preprocessed\"].tolist() ])\n",
    "                    , predict( [ ' '.join(tweet_tokens)  for tweet_tokens in  test_tweets_df[\"preprocessed\"].tolist() ])]).T\n",
    "test_df.columns = ['isOffensive' , 'isOffensive_pred_prob' ,  'isOffensive_pred' ]\n",
    "test_df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing Precision and Recall\n",
    "precision = precision_score( test_tweets_df['isOffensive'].tolist()  , predict( [ ' '.join(tweet_tokens)  for tweet_tokens in  test_tweets_df[\"preprocessed\"].tolist() ]))\n",
    "recall = recall_score( test_tweets_df['isOffensive'].tolist()  , predict( [ ' '.join(tweet_tokens)  for tweet_tokens in  test_tweets_df[\"preprocessed\"].tolist() ]))\n",
    "print('precision : ' , (\"%.2f\" %  precision ))\n",
    "print('recall : ' , (\"%.2f\" %  recall ) )\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
